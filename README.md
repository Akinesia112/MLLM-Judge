<div align="center">
<h1>MLLM-as-a-Judge:
Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark</h1>

[![Website](https://img.shields.io/badge/Website-%F0%9F%8C%8D-blue?style=for-the-badge&logoWidth=40)](https://mllm-judge.github.io/)
[![Paper](https://img.shields.io/badge/Paper-%F0%9F%8E%93-lightgrey?style=for-the-badge&logoWidth=40)](https://arxiv.org/abs/2402.04788)
[![Dataset](https://img.shields.io/badge/Dataset-%F0%9F%92%BE-green?style=for-the-badge&logoWidth=40)](https://huggingface.co/datasets/shuaishuaicdp/MLLM-Judge)
[![Leaderboard](https://img.shields.io/badge/Leaderboard-%F0%9F%9A%80-brightgreen?style=for-the-badge&logoWidth=40)](https://mllm-judge.github.io/leaderboard.html)


<img src="https://img.shields.io/github/last-commit/Dongping-Chen/MLLM-Judge?style=flat-square&color=5D6D7E" alt="git-last-commit" />
<img src="https://img.shields.io/github/commit-activity/m/Dongping-Chen/MLLM-Judge?style=flat-square&color=5D6D7E" alt="GitHub commit activity" />
<img src="https://img.shields.io/github/languages/top/Dongping-Chen/MLLM-Judge?style=flat-square&color=5D6D7E" alt="GitHub top language" />

<img src="Figures/fig1.png">
<img src="Figures/Radar.png">
<p align="center">

</p>
</div>

## Updates & News
- [01/06/2024] :star: Our paper is accepted by ICML 2024 Oral!
- [20/03/2024] :star: We release our complete dataset with guidelines and scripts for benchmarking current VLMs!
- [14/02/2024] :page_facing_up: We release our paper on [Arxiv](http://arxiv.org/abs/2402.04788) today!
  
## Contents
- [Updates \& News](#updates--news)
- [Contents](#contents)
- [Benchmark:MLLM-as-a-Judge](#benchmarkmllm-as-a-judge)
- [Benchmark mainstream MLLMs](#benchmark-mainstream-mllms)
  - [via API](#via-api)
- [Contributing](#contributing)
- [Acknowledgments](#acknowledgments)
- [Citation](#citation)
  
## Benchmark:MLLM-as-a-Judge
This benchmark is structured into three main components: images, the main dataset, and sub-datasets. The arrangement is as follows:

```markdown
/MLLM-Judge
├── Figures (images for github repository)
├── Datasets
│   ├── Images (images for Benchmark)
│   ├── Benchmark
│   │   ├── batch.jsonl
│   │   ├── pair.jsonl
│   │   └── score.jsonl
│   │
│   └── raw_data
│       ├── step1
│       ├── step2
│       └── step3
│       
└── Hard & HQ
    ├── Hard
    └── HQ
```

1. **Figures**: Contains images for the GitHub repository. These images are used to illustrate and explain the contents of the repository, aiding users in better understanding the project.

2. **Dataset/**: This part of the dataset is developed in three steps, mirroring the structure outlined in our article. It includes MLLM outputs under three different settings: Scoring Evaluation, Pair Comparison, and Batch Ranking. Additionally, this section encompasses human annotation results and agreement data. In Scoring Evaluation, we also include responses data in a verbose setting for our ablation study.
   - **Benchmark**: The Final dataset with human annotations used as a benchmark to assess model performance. These annotations provide a reliable reference to verify if the model's judgments align with human evaluations.
   - **`raw_data/step1`**: Contains original image-instruction pairs selected from 10 datasets. This is the starting point for data processing and model training, containing the initial input data.
   - **`raw_data/step2`**: Contains response data generated by four different MLLMs. This step aims to enrich the dataset and increase its diversity by generating data through multiple models.
   - **`raw_data/step3`**: Divides the data from step2 into three parts, each under different settings, containing responses from various MLLM Judges. This helps analyze and compare the performance differences across models under the same tasks.
   

3. **`Dataset/Hard & HQ`**: Contains two specially curated datasets for specific data analysis and model training purposes:
   - **Hard**: Includes samples considered difficult under three different settings. This data is used to test and improve MLLM capabilities in dealing with complex scenarios.
   - **HQ (High Quality)**: Contains samples where the MLLM-as-a-Judge performed well. These high-quality samples help understand under what conditions the model performs best.

4. **`Dataset/image`**: All images utilized in our study are contained in this section. You can download all images by cloning this repository. 


## Benchmark mainstream MLLMs

### via API
We benchmark GPT-4V(ision), Gemini, Qwen, LLaVA-1.6 via API. You can replicate our experiment result by running the following scripts:
```shell
python scripts/api_benchmark.py \
--model <> \ # 'gemini', 'gpt-4v', 'gpt-4o', 'llava-1.6-34b', 'llava-1.6-13b', 'llava-1.6-7b', 'qwen-vl-plus', 'qwen-vl-max', 'qwen-vl-chat'
--judge_mode <> \ # 'score', 'batch', 'pair'
--temperature <> \ # default as 0.4
--top_p <> \ # default as 0.2
--image_root <path to image> \ 
--setting <> \ # ablation study for COT, default as No COT

```

For example:

```
python api_benchmark.py   --model "gpt-4o"   --top_p 0.2   --temperature 0.4   --judge_mode "score"   --image_root "./MLLM-Judge/Dataset/image"   --setting "No COT" --api "sk-proj-XXX"
```

<!-- ### local models -->

### Collect Judgments from MLLMs
#### GPT-4V(ision)
You can run the following script in shell to collect GPT-4V's judgement:
```shell
# Batch evaluation in No COT settings
python gpt4_judge.py --input_json '<your_path>/MLLM-Judge/Dataset/Benchmark/Batch.jsonl' --output_json './Batch.jsonl'  --image_root '<your_path>/MLLM-Judge/images' --evaluate 'Batch' --setting 'No COT'
# Score evaluation in No COT settings
python gpt4_judge.py --input_json
'<your_path>/MLLM-Judge/Dataset/Benchmark/Score.jsonl' --output_json './Score.jsonl'  --image_root '<your_path>/MLLM-Judge/images' --evaluate 'Score' --setting 'No COT'
# Pair Comparison in No COT settings
python gpt4_judge.py --input_json
'<your_path>/MLLM-Judge/Dataset/Benchmark/Pair.jsonl' --output_json './Pair.jsonl'  --image_root '<your_path>/MLLM-Judge/images' --evaluate 'Pair' --setting 'No COT'
```
#### LLaVA
You should first follow instructions in [LLaVA's repository](https://github.com/haotian-liu/LLaVA) to download llava-v1.5-13b and create a new python environment called LLaVA. Then, you need to move `\scripts\llava_inference.py` to `<your path>/LLaVA` and move to the `<your path>/LLaVA`. Then, you can run the following to produce MLLM's judging result:
```shell
# Batch evaluation in No COT settings
python llava_judge.py --input_json '<your_path>/MLLM-Judge/Dataset/Benchmark/Batch.jsonl' --output_json './Batch.jsonl'  --image_root '<your_path>/MLLM-Judge/images' --evaluate 'Batch' --setting 'No COT'
# Score evaluation in No COT settings
python llava_judge.py --input_json
'<your_path>/MLLM-Judge/Dataset/Benchmark/Score.jsonl' --output_json './Score.jsonl'  --image_root '<your_path>/MLLM-Judge/images' --evaluate 'Score' --setting 'No COT'
# Pair Comparison in No COT settings
python llava_judge.py --input_json
'<your_path>/MLLM-Judge/Dataset/Benchmark/Pair.jsonl' --output_json './Pair.jsonl'  --image_root '<your_path>/MLLM-Judge/images' --evaluate 'Pair' --setting 'No COT'
```
#### Gemini
You should first register your google account to get a Gemini-Pro API or run the script in Colab Pro.
To collect judging results from Gemini-Pro-Vision, you should run the following scripts in shell:
```shell
# Batch evaluation in No COT settings
python gemini_judge.py --api 'your_api' --input_json '<your_path>/MLLM-Judge/Dataset/Benchmark/Batch.jsonl' --output_json './Batch.jsonl'  --image_root '<your_path>/MLLM-Judge/images' --evaluate 'Batch' --setting 'No COT'
# Score evaluation in No COT settings
python gemini_judge.py --api 'your_api' --input_json '<your_path>/MLLM-Judge/Dataset/Benchmark/Score.jsonl' --output_json './Score.jsonl'  --image_root '<your_path>/MLLM-Judge/images' --evaluate 'Batch' --setting 'No COT'
# Pair Comparison in No COT settings
python gemini_judge.py --api 'your_api' --input_json '<your_path>/MLLM-Judge/Dataset/Benchmark/Pair.jsonl' --output_json './Pair.jsonl'  --image_root '<your_path>/MLLM-Judge/images' --evaluate 'Batch' --setting 'No COT'
```
Notice: If you run Gemini in your local environment, the inference limitation is very severe, reaching only 60 QPM.
#### CogVLM
You should follow the instruction in [CogVLM's repository](https://github.com/THUDM/CogVLM) to download CogVLM checkpoint. Then, you should move `scripts/cogvlm_judge.py` to `<your_path>/CogVLM` to collect judgment from CogVLM, using the following scripts in shell:
```shell
# Score evaluation in No COT settings
python cogvlm_judge.py --api 'your_api' --input_json '<your_path>/MLLM-Judge/Dataset/Benchmark/Score.jsonl' --output_json './Score.jsonl'  --image_root '<your_path>/MLLM-Judge/images' --evaluate 'Batch' --setting 'No COT'
# Pair Comparison in No COT settings
python cogvlm_judge.py --api 'your_api' --input_json '<your_path>/MLLM-Judge/Dataset/Benchmark/Pair.jsonl' --output_json './Pair.jsonl'  --image_root '<your_path>/MLLM-Judge/images' --evaluate 'Batch' --setting 'No COT'
```
Notice: CogVLM cannot follow our output template in Batch Evaluation setting.

## Contributing
Contributions to this project are welcome. Please consider the following ways to contribute:

- Reporting issues
- Proposing new features or improvements
- Benchmark other mainstream MLLMs

## Acknowledgments

This project is based on the findings and methodologies presented in the paper [LLM-as-a-Judge](https://arxiv.org/abs/2306.05685) and [HallusionBench](https://arxiv.org/abs/2310.14566).

## Citation

```
@misc{chen2024mllmasajudge,
      title={MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark}, 
      author={Dongping Chen and Ruoxi Chen and Shilin Zhang and Yinuo Liu and Yaochen Wang and Huichi Zhou and Qihui Zhang and Pan Zhou and Yao Wan and Lichao Sun},
      year={2024},
      eprint={2402.04788},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```
