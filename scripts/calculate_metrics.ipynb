{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a demo code for calculating metrics for the MLLM-as-a-Judge paper. You can transform your results in a *.jsonl* file, and then use this code to calculate the metrics.\n",
    "\n",
    "For *Batch* setting, we use Levenshtein Distance to calculate the distance between the predicted and the target.\n",
    "\n",
    "For *Pair* setting, we include code for Tie and Non-Tie cases, and treat it as a classification problem. The metric is *Accuracy*.\n",
    "\n",
    "For *Score* setting, we use *Pearson Similarity* to calculate the similarity between the predicted and the target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Ranking (Levenshtein Distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from Levenshtein import distance\n",
    "\n",
    "def calculate_levenshtein_metrics(pred_file, gt_file):\n",
    "    \"\"\"\n",
    "    Calculate edit distance metrics between predictions and ground truth\n",
    "    \n",
    "    Args:\n",
    "        pred_file: Path to predictions file\n",
    "        gt_file: Path to ground truth file\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing metrics per dataset and overall average\n",
    "    \"\"\"\n",
    "    # Load ground truth\n",
    "    gt_data = {}\n",
    "    with open(gt_file, 'r') as f:\n",
    "        for line in f:\n",
    "            item = json.loads(line)\n",
    "            gt_data[item['id']] = {\n",
    "                'dataset': item['original_dataset'],\n",
    "                'answers': item['human']\n",
    "            }\n",
    "            \n",
    "    # Load predictions\n",
    "    pred_data = {}\n",
    "    with open(pred_file, 'r') as f:\n",
    "        for line in f:\n",
    "            item = json.loads(line)\n",
    "            pred_data[item['id']] = item['human']\n",
    "            \n",
    "    # Calculate distances per dataset\n",
    "    dataset_distances = {}\n",
    "    for id in gt_data:\n",
    "        if id not in pred_data:\n",
    "            continue\n",
    "            \n",
    "        dataset = gt_data[id]['dataset']\n",
    "        if dataset not in dataset_distances:\n",
    "            dataset_distances[dataset] = []\n",
    "            \n",
    "        # Calculate distance to ground truth answer\n",
    "        dist = distance(pred_data[id], gt_data[id]['answers'])\n",
    "        dataset_distances[dataset].append(dist)\n",
    "    \n",
    "    # Calculate averages\n",
    "    metrics = {}\n",
    "    all_distances = []\n",
    "    \n",
    "    for dataset in dataset_distances:\n",
    "        avg = np.mean(dataset_distances[dataset])\n",
    "        metrics[dataset] = {\n",
    "            'average_distance': avg,\n",
    "            'num_samples': len(dataset_distances[dataset])\n",
    "        }\n",
    "        all_distances.extend(dataset_distances[dataset])\n",
    "        \n",
    "    metrics['overall'] = {\n",
    "        'average_distance': np.mean(all_distances),\n",
    "        'num_samples': len(all_distances)\n",
    "    }\n",
    "    \n",
    "    return metrics \n",
    "\n",
    "# Example usage:\n",
    "\n",
    "pred_file = '<your_prediction_file>'  # User needs to provide prediction file path\n",
    "gt_file = '../Dataset/Benchmark/batch.jsonl'   # User needs to provide ground truth file path\n",
    "\n",
    "metrics = calculate_levenshtein_metrics(pred_file, gt_file)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nResults per dataset:\")\n",
    "for dataset in metrics:\n",
    "    if dataset != 'overall':\n",
    "        print(f\"\\n{dataset}:\")\n",
    "        print(f\"Average Levenshtein distance: {metrics[dataset]['average_distance']:.2f}\")\n",
    "        print(f\"Number of samples: {metrics[dataset]['num_samples']}\")\n",
    "        \n",
    "print(f\"\\nOverall average distance: {metrics['overall']['average_distance']:.2f}\")\n",
    "print(f\"Total samples: {metrics['overall']['num_samples']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pair Comparison (Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def calculate_accuracy_metrics(pred_file, gt_file, include_ties=False):\n",
    "    \"\"\"\n",
    "    Calculate accuracy metrics between predictions and ground truth\n",
    "    \n",
    "    Args:\n",
    "        pred_file: Path to predictions file\n",
    "        gt_file: Path to ground truth file\n",
    "        include_ties: Whether to include samples with tie (C) answers in calculation\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing metrics per dataset and overall average\n",
    "    \"\"\"\n",
    "    # Load ground truth\n",
    "    gt_data = {}\n",
    "    with open(gt_file, 'r') as f:\n",
    "        for line in f:\n",
    "            item = json.loads(line)\n",
    "            gt_data[item['id']] = {\n",
    "                'dataset': item['original_dataset'],\n",
    "                'answer': item['human']\n",
    "            }\n",
    "            \n",
    "    # Load predictions\n",
    "    pred_data = {}\n",
    "    with open(pred_file, 'r') as f:\n",
    "        for line in f:\n",
    "            item = json.loads(line)\n",
    "            pred_data[item['id']] = item['human']\n",
    "            \n",
    "    # Calculate accuracies per dataset\n",
    "    dataset_accuracies = {}\n",
    "    for id in gt_data:\n",
    "        if id not in pred_data:\n",
    "            continue\n",
    "            \n",
    "        # Skip ties if include_ties is False\n",
    "        if not include_ties and (gt_data[id]['answer'] == 'C' or pred_data[id] == 'C'):\n",
    "            continue\n",
    "            \n",
    "        dataset = gt_data[id]['dataset']\n",
    "        if dataset not in dataset_accuracies:\n",
    "            dataset_accuracies[dataset] = {\n",
    "                'correct': 0,\n",
    "                'total': 0\n",
    "            }\n",
    "            \n",
    "        # Compare predictions with ground truth\n",
    "        if pred_data[id] == gt_data[id]['answer']:\n",
    "            dataset_accuracies[dataset]['correct'] += 1\n",
    "        dataset_accuracies[dataset]['total'] += 1\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {}\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for dataset in dataset_accuracies:\n",
    "        correct = dataset_accuracies[dataset]['correct']\n",
    "        total = dataset_accuracies[dataset]['total']\n",
    "        \n",
    "        accuracy = correct / total if total > 0 else float('nan')\n",
    "            \n",
    "        metrics[dataset] = {\n",
    "            'accuracy': accuracy,\n",
    "            'num_samples': total\n",
    "        }\n",
    "        \n",
    "        total_correct += correct\n",
    "        total_samples += total\n",
    "        \n",
    "    # Calculate overall accuracy\n",
    "    metrics['overall'] = {\n",
    "        'accuracy': total_correct / total_samples if total_samples > 0 else float('nan'),\n",
    "        'num_samples': total_samples\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Example usage:\n",
    "pred_file = '../Dataset/Benchmark/pair.jsonl'  # User needs to provide prediction file path\n",
    "gt_file = '../Dataset/Benchmark/pair.jsonl'   # User needs to provide ground truth file path\n",
    "\n",
    "# Calculate metrics excluding ties\n",
    "metrics_no_ties = calculate_accuracy_metrics(pred_file, gt_file, include_ties=False)\n",
    "\n",
    "print(\"\\nResults excluding ties:\")\n",
    "print(\"\\nResults per dataset:\")\n",
    "for dataset in metrics_no_ties:\n",
    "    if dataset != 'overall':\n",
    "        print(f\"\\n{dataset}:\")\n",
    "        print(f\"Accuracy: {metrics_no_ties[dataset]['accuracy']:.3f}\")\n",
    "        print(f\"Number of samples: {metrics_no_ties[dataset]['num_samples']}\")\n",
    "        \n",
    "print(f\"\\nOverall accuracy: {metrics_no_ties['overall']['accuracy']:.3f}\")\n",
    "print(f\"Total samples: {metrics_no_ties['overall']['num_samples']}\")\n",
    "\n",
    "# Calculate metrics including ties\n",
    "metrics_with_ties = calculate_accuracy_metrics(pred_file, gt_file, include_ties=True)\n",
    "\n",
    "print(\"\\nResults including ties:\")\n",
    "print(\"\\nResults per dataset:\")\n",
    "for dataset in metrics_with_ties:\n",
    "    if dataset != 'overall':\n",
    "        print(f\"\\n{dataset}:\")\n",
    "        print(f\"Accuracy: {metrics_with_ties[dataset]['accuracy']:.3f}\")\n",
    "        print(f\"Number of samples: {metrics_with_ties[dataset]['num_samples']}\")\n",
    "        \n",
    "print(f\"\\nOverall accuracy: {metrics_with_ties['overall']['accuracy']:.3f}\")\n",
    "print(f\"Total samples: {metrics_with_ties['overall']['num_samples']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score (Pearson Similarity)\n",
    "You can also add more metrics to your results such as cosine similarity, MSE, MAE, Spearman's rank correlation, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "import json\n",
    "\n",
    "def calculate_pearson_metrics(pred_file, gt_file):\n",
    "    \"\"\"\n",
    "    Calculate Pearson correlation metrics between predictions and ground truth scores\n",
    "    \n",
    "    Args:\n",
    "        pred_file: Path to predictions file\n",
    "        gt_file: Path to ground truth file\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing metrics per dataset and overall average\n",
    "    \"\"\"\n",
    "    # Load ground truth\n",
    "    gt_data = {}\n",
    "    with open(gt_file, 'r') as f:\n",
    "        for line in f:\n",
    "            item = json.loads(line)\n",
    "            gt_data[item['id']] = {\n",
    "                'dataset': item['original_dataset'],\n",
    "                'score': float(item['human'])  # Convert string score to float\n",
    "            }\n",
    "            \n",
    "    # Load predictions\n",
    "    pred_data = {}\n",
    "    with open(pred_file, 'r') as f:\n",
    "        for line in f:\n",
    "            item = json.loads(line)\n",
    "            pred_data[item['id']] = float(item['human'])  # Convert string score to float\n",
    "            \n",
    "    # Calculate correlations per dataset\n",
    "    dataset_scores = {}\n",
    "    for id in gt_data:\n",
    "        if id not in pred_data:\n",
    "            continue\n",
    "            \n",
    "        dataset = gt_data[id]['dataset']\n",
    "        if dataset not in dataset_scores:\n",
    "            dataset_scores[dataset] = {\n",
    "                'pred': [],\n",
    "                'gt': []\n",
    "            }\n",
    "            \n",
    "        dataset_scores[dataset]['pred'].append(pred_data[id])\n",
    "        dataset_scores[dataset]['gt'].append(gt_data[id]['score'])\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {}\n",
    "    all_pred = []\n",
    "    all_gt = []\n",
    "    \n",
    "    for dataset in dataset_scores:\n",
    "        corr, p_value = pearsonr(dataset_scores[dataset]['pred'], dataset_scores[dataset]['gt'])\n",
    "        metrics[dataset] = {\n",
    "            'correlation': corr,\n",
    "            'p_value': p_value,\n",
    "            'num_samples': len(dataset_scores[dataset]['pred'])\n",
    "        }\n",
    "        all_pred.extend(dataset_scores[dataset]['pred'])\n",
    "        all_gt.extend(dataset_scores[dataset]['gt'])\n",
    "        \n",
    "    # Calculate overall correlation\n",
    "    overall_corr, overall_p = pearsonr(all_pred, all_gt)\n",
    "    metrics['overall'] = {\n",
    "        'correlation': overall_corr,\n",
    "        'p_value': overall_p,\n",
    "        'num_samples': len(all_pred)\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# %% Example usage\n",
    "\n",
    "pred_file = '../Dataset/Benchmark/score.jsonl'  # User needs to provide prediction file path\n",
    "gt_file = '../Dataset/Benchmark/score.jsonl'   # User needs to provide ground truth file path\n",
    "\n",
    "# Calculate Pearson correlation metrics\n",
    "pearson_metrics = calculate_pearson_metrics(pred_file, gt_file)\n",
    "\n",
    "print(\"\\nPearson Correlation Results:\")\n",
    "print(\"\\nResults per dataset:\")\n",
    "for dataset in pearson_metrics:\n",
    "    if dataset != 'overall':\n",
    "        print(f\"\\n{dataset}:\")\n",
    "        print(f\"Correlation: {pearson_metrics[dataset]['correlation']:.3f}\")\n",
    "        print(f\"P-value: {pearson_metrics[dataset]['p_value']:.3e}\")\n",
    "        print(f\"Number of samples: {pearson_metrics[dataset]['num_samples']}\")\n",
    "        \n",
    "print(f\"\\nOverall correlation: {pearson_metrics['overall']['correlation']:.3f}\")\n",
    "print(f\"Overall p-value: {pearson_metrics['overall']['p_value']:.3e}\")\n",
    "print(f\"Total samples: {pearson_metrics['overall']['num_samples']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cdp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
