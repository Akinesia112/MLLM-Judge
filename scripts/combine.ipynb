{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def merge_and_sort_jsonl_files(directory_path, output_file):\n",
    "    all_data = []\n",
    "\n",
    "    # 遍历给定路径下的所有文件\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.jsonl'):\n",
    "            # 为每个jsonl文件读取和解析每一行\n",
    "            with open(os.path.join(directory_path, filename), 'r') as file:\n",
    "                for line in file:\n",
    "                    all_data.append(json.loads(line))\n",
    "    \n",
    "    # 根据'pair_id'关键字对所有数据进行排序\n",
    "    sorted_data = sorted(all_data, key=lambda x: x['pair_id'])\n",
    "    print(len(all_data))\n",
    "    # 将排序后的数据写入新的json文件\n",
    "    with open(output_file, 'w') as file:\n",
    "        json.dump(sorted_data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "# 用法示例\n",
    "merge_and_sort_jsonl_files('/media/sata2/cdp/backup/MLLM-Judge-backup/Dataset/Benchmark/arxiv_data/pair', 'pair.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def convert_json_to_jsonl(input_json_path, output_jsonl_path):\n",
    "    # 读取JSON文件\n",
    "    with open(input_json_path, 'r', encoding='utf-8') as json_file:\n",
    "        data = json.load(json_file)  # 假设这是一个JSON对象数组\n",
    "\n",
    "    # 确保读取到的数据是一个列表\n",
    "    if not isinstance(data, list):\n",
    "        raise ValueError(\"Expected a JSON array in the input file.\")\n",
    "\n",
    "    # 将每个JSON对象写入JSONL文件的单独一行\n",
    "    with open(output_jsonl_path, 'w', encoding='utf-8') as jsonl_file:\n",
    "        for item in data:\n",
    "            jsonl_file.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "# 使用示例\n",
    "convert_json_to_jsonl('/media/ssd/cdp/MLLM-Judge/Dataset/Benchmark/pair.json', 'pair.jsonl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/media/ssd/cdp/MLLM-Judge/Dataset/step3/Score/Gemini.jsonl\", 'r') as f:\n",
    "    data = [json.loads(line) for line in f]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data:\n",
    "    if int(i['result']['judge']) > 5:\n",
    "        print(i['result'])\n",
    "        i['result']['judge'] = str(i['result']['judge'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将修改后的数据写入新的json文件\n",
    "with open(\"/media/ssd/cdp/MLLM-Judge/Dataset/step3/Score/Gemini.jsonl\", 'w') as file:\n",
    "    for i in data:\n",
    "        json.dump(i, file, ensure_ascii=False)\n",
    "        file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5696\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5745"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('/media/ssd/cdp/MLLM-Judge/Dataset/Benchmark/pair.jsonl', 'r') as f:\n",
    "    gt = [json.loads(line) for line in f]\n",
    "\n",
    "with open('/media/ssd/cdp/MLLM-Judge/Dataset/step3/Pair/Gemini.jsonl', 'r') as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "    \n",
    "print(len(gt))\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data:\n",
    "    if 'result' not in list(i.keys()):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4511\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "reversed = 0\n",
    "for i in gt:\n",
    "    for j in data:\n",
    "        if i['pair_id'] == j['pair_id']:\n",
    "            if i['answer1']['name'] != j['answer1']['name'] or i['answer2']['name'] != j['answer2']['name']:\n",
    "                cnt+=1\n",
    "            if i['answer1']['name'] == i['answer2']['name'] or j['answer2']['name'] == j['answer1']['name']:\n",
    "                reversed+=1\n",
    "print(cnt)\n",
    "print(reversed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'result'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[115], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m j[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer1\u001b[39m\u001b[38;5;124m'\u001b[39m], j[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer2\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m j[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer2\u001b[39m\u001b[38;5;124m'\u001b[39m], j[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer1\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# 交换answers\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# 根据匹配调整result字段\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m j[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjudge\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mj\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresult\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjudge\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m'\u001b[39m   \n\u001b[1;32m     15\u001b[0m revised_data\u001b[38;5;241m.\u001b[39mappend(j)\n\u001b[1;32m     16\u001b[0m found_match \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# 找到匹配，将退出内层循环\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'result'"
     ]
    }
   ],
   "source": [
    "revised_data = []\n",
    "for j in data:\n",
    "    found_match = False  # 标记是否找到匹配\n",
    "    for i in gt:\n",
    "        if i['id'] == j['id']:\n",
    "            if i['answer1']['name'] == j['answer1']['name'] and i['answer2']['name'] == j['answer2']['name']:\n",
    "                j['pair_id'] = i['pair_id']\n",
    "                revised_data.append(j)\n",
    "                found_match = True  # 找到匹配，将退出内层循环\n",
    "            elif i['answer1']['name'] == j['answer2']['name'] and i['answer2']['name'] == j['answer1']['name']:\n",
    "                j['pair_id'] = i['pair_id']\n",
    "                j['answer1'], j['answer2'] = j['answer2'], j['answer1']  # 交换answers\n",
    "                # 根据匹配调整result字段\n",
    "                j['result']['judge'] = 'A' if j['result']['judge'] == 'B' else 'B'   \n",
    "                revised_data.append(j)\n",
    "                found_match = True  # 找到匹配，将退出内层循环\n",
    "        if found_match:\n",
    "            break  # 退出内层循环\n",
    "                \n",
    "len(revised_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rerank the revised_data by pair_id\n",
    "revised_data = sorted(revised_data, key=lambda x: x['pair_id'])\n",
    "\n",
    "# write revised_data to a new jsonl file\n",
    "with open(\"/media/ssd/cdp/MLLM-Judge/Dataset/step3/Pair/new CogVLM.jsonl\", 'w') as file:\n",
    "    for i in revised_data:\n",
    "        json.dump(i, file, ensure_ascii=False)\n",
    "        file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6151\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def check_duplicate_pair_ids(jsonl_path):\n",
    "    pair_id_counts = {}\n",
    "    duplicates = []\n",
    "\n",
    "    # 读取JSONL文件并统计pair_id出现的次数\n",
    "    with open(jsonl_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            pair_id = data.get('pair_id')\n",
    "            if pair_id in pair_id_counts:\n",
    "                pair_id_counts[pair_id] += 1\n",
    "            else:\n",
    "                pair_id_counts[pair_id] = 1\n",
    "\n",
    "    # 重新遍历文件，查找具有相同pair_id的元素\n",
    "    with open(jsonl_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            pair_id = data.get('pair_id')\n",
    "            # 如果pair_id的计数超过1，则记录下来\n",
    "            if pair_id_counts[pair_id] > 1 and pair_id not in duplicates:\n",
    "                print(f\"Duplicate pair_id found: {pair_id} with data: {data}\")\n",
    "                duplicates.append(pair_id)\n",
    "    print(len(pair_id_counts.items()))\n",
    "\n",
    "# 使用示例\n",
    "check_duplicate_pair_ids('/media/ssd/cdp/MLLM-Judge/Dataset/step3/Pair/GPT-4V(ision).jsonl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def remove_duplicate_pair_ids(jsonl_path):\n",
    "    seen_pair_ids = set()\n",
    "    unique_entries = []\n",
    "\n",
    "    # 读取并处理JSONL文件，只保留第一个出现的具有特定pair_id的条目\n",
    "    with open(jsonl_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            pair_id = data.get('pair_id')\n",
    "            if pair_id not in seen_pair_ids:\n",
    "                seen_pair_ids.add(pair_id)\n",
    "                unique_entries.append(data)\n",
    "\n",
    "    # 将处理后的数据写回原文件\n",
    "    with open(jsonl_path, 'w', encoding='utf-8') as file:\n",
    "        for entry in unique_entries:\n",
    "            file.write(json.dumps(entry, ensure_ascii=False) + '\\n')\n",
    "\n",
    "# 使用示例\n",
    "remove_duplicate_pair_ids('/media/ssd/cdp/MLLM-Judge/Dataset/Benchmark/pair.jsonl')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
